<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>拉普拉斯平滑和M-估计 | LawrencePeng&#39;s Blog | 专注收集代码小精灵</title>

  
  <meta name="author" content="LawrencePeng">
  

  
  <meta name="description" content="Meow">
  

  
  
  <meta name="keywords" content="Math,Machine Learning">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="拉普拉斯平滑和M-估计"/>

  <meta property="og:site_name" content="LawrencePeng&#39;s Blog"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="LawrencePeng&#39;s Blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">LawrencePeng&#39;s Blog</a>
    </h1>
    <p class="site-description">专注收集代码小精灵</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>拉普拉斯平滑和M-估计</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/06/02/拉普拉斯平滑和M-估计/" rel="bookmark">
        <time class="entry-date published" datetime="2017-06-01T16:24:09.000Z">
          2017-06-02
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <ul>
<li>目的<ul>
<li>某事件发生的频数过低时使用MLE会导致极差的估计偏差，拉普拉斯平滑和M-估计都是为了解决这一问题。</li>
</ul>
</li>
<li>拉普拉斯平滑<ul>
<li>相当于假设均匀分布的先验的情况下和后验做blending，实际上并没有如此简单。但是这种解释有诸多不合理，想要了解其中的原理可以看看<a href="https://zhuanlan.zhihu.com/p/24291822" target="_blank" rel="external">这篇文章</a>。我花了一个小时才看懂，发现概率论得好好补补了。。。</li>
</ul>
</li>
<li>M-估计<ul>
<li>P=(r + Pa * m)/(n + m),其中m为估计的值。</li>
<li>或者P=(n/(n+m)) <em> (r/n) + (m/(n+m)) </em> (Pa)</li>
<li>不难看出其实对MLE和拉普拉斯平滑的一种综合。</li>
</ul>
</li>
</ul>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Math/">Math</a><a href="/tags/Machine-Learning/">Machine Learning</a>
    </span>
    

    </div>

    
  </div>
</article>


    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2017 LawrencePeng
    
  </p>
</footer>
    
  </div>
</div>
</body>
</html>